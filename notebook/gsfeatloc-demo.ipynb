{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Project       : GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting\n",
    "# File          : gsfeatloc-demo.ipynb\n",
    "# Description   : This is the demo notebook for GSFeatLoc.\n",
    "# \n",
    "# Author        : Jongwon Lee (jongwon5@illinois.edu)\n",
    "# Year          : 2025\n",
    "# License       : BSD License\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerfbaselines import new_cameras, camera_model_to_int\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import time\n",
    "import os\n",
    "import pycolmap\n",
    "\n",
    "# Import the utils module from the gs-loc package\n",
    "\n",
    "# gsfeatloc is located in the parent directory of this script\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from gsfeatloc.utils import get_world_frame_difference, perturb_SE3\n",
    "from gsfeatloc.visualizer import visualize_matches, visualize_feature_points, visualize_3d_points\n",
    "from gsfeatloc.feature_matcher import do_feature_matching_SIFT, do_feature_matching_SPSG, do_feature_matching_LoFTR\n",
    "from gsfeatloc.dataset_loader import load_frames_blender, load_frames_colmap\n",
    "from gsfeatloc.pose_estimator import get_2d_feature_points, compute_3d_points, estimate_camera_pose\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "rng = np.random.default_rng(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models and files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained Gsplat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the ExitStack to manage the context managers such that\n",
    "# we can persist the contexts between cells. This is rarely needed\n",
    "# in practice, but it is useful for this tutorial.\n",
    "from contextlib import ExitStack\n",
    "stack = ExitStack().__enter__()\n",
    "\n",
    "import pprint\n",
    "from nerfbaselines import load_checkpoint\n",
    "\n",
    "dataset = \"blender\"\n",
    "sequence = \"lego\"\n",
    "downscale_factor = 1\n",
    "\n",
    "scene_path = f\"/home/jongwonlee/datasets/nerfbaselines/{dataset}/{sequence}\"\n",
    "checkpoint_path = f\"/home/jongwonlee/models/gsplat/{dataset}/{sequence}/checkpoint-30000\"\n",
    "reading_dir = \"test\" if \"blender\" is dataset else f\"images_{downscale_factor}\" if downscale_factor > 1 else \"images\"\n",
    "\n",
    "# Start the docker backend and load the checkpoint\n",
    "model, _ = stack.enter_context(load_checkpoint(checkpoint_path, backend=\"docker\"))\n",
    "\n",
    "# Print model information\n",
    "pprint.pprint(model.get_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the metadata with ground-truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a frame's filename, its ground-truth pose, and camera parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the source of test data\n",
    "colmap_sparse_path = Path(scene_path, \"sparse\", \"0\")\n",
    "test_list_path = Path(scene_path, \"test_list.txt\")\n",
    "transforms_test_path = Path(scene_path, \"transforms_test.json\")\n",
    "\n",
    "if colmap_sparse_path.exists():\n",
    "    print(f\"Using test data from COLMAP reconstruction in {colmap_sparse_path}\")\n",
    "    K, w, h, frames = load_frames_colmap(colmap_sparse_path, downscale_factor)\n",
    "\n",
    "    if not test_list_path.exists():\n",
    "        raise FileNotFoundError(f\"test_list.txt not found in {scene_path}. Please provide a valid path to the dataset.\")\n",
    "\n",
    "    # Load and filter frames based on the test list\n",
    "    with open(test_list_path, \"r\") as f:\n",
    "        test_list = f.read().splitlines()\n",
    "        test_list_without_ext = [os.path.splitext(filename)[0] for filename in test_list]\n",
    "    \n",
    "    frames = {filename: frames[filename] for filename in test_list_without_ext if filename in frames}\n",
    "    print(f\"Total number of frames: {len(frames)}\")\n",
    "\n",
    "elif transforms_test_path.exists():\n",
    "    print(f\"Using test data from {transforms_test_path}\")\n",
    "    K, w, h, frames = load_frames_blender(transforms_test_path)\n",
    "    print(f\"Total number of frames: {len(frames)}\")\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No test data found in {scene_path}. Please provide a valid path to the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a frame from the test set\n",
    "filenames = sorted(list(frames.keys()))\n",
    "filename = rng.choice(filenames)\n",
    "filename = \"r_75\"\n",
    "print(f\"Selected frame: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the query image (with the ground-truth pose T_inW_ofC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_query = np.array(Image.open(Path(scene_path, reading_dir, filename + \".JPG\"))) if dataset == \"mipnerf360\" else \\\n",
    "    np.array(Image.open(Path(scene_path, reading_dir, filename + \".png\")))\n",
    "# depth_query_raw = np.array(Image.open(Path(scene_path, filename + \"_depth_0001.png\")), dtype=np.float32)[...,0]\n",
    "\n",
    "# Show the image\n",
    "# plt.gcf().set_size_inches(4, 4)\n",
    "plt.imshow(im_query)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual image size may be different from the image size being loaded (by up to two pixels)\n",
    "# If that's the case, re-assign h and w to mach the image size\n",
    "\n",
    "assert abs(im_query.shape[0] - h) <= 2, f\"Image height mismatch: {im_query.shape[0]} vs {h}\"\n",
    "assert abs(im_query.shape[1] - w) <= 2, f\"Image width mismatch: {im_query.shape[1]} vs {w}\"\n",
    "\n",
    "if im_query.shape[0] != h or im_query.shape[1] != w:\n",
    "    print(f\"[Warning] Image size mismatch: {im_query.shape[:2]} vs ({h}, {w})\")\n",
    "    h, w = im_query.shape[:2]\n",
    "    print(f\"[Warning] Updated image size: {h}, {w}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perturb the camera pose. This will serve as a reference pose of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturb the camera pose\n",
    "M = 12.10  # Rotation perturbation in degrees\n",
    "N = 0.87  # Translation perturbation in meters\n",
    "\n",
    "T_inW_ofC = frames[filename]\n",
    "\n",
    "T_inW_ofC_perturbated = perturb_SE3(T_inW_ofC, \n",
    "                                    rotation_magnitude=30.00, \n",
    "                                    translation_magnitude=0.20, \n",
    "                                    axis_mode='random', magnitude_mode='gaussian', \n",
    "                                    rng=rng)\n",
    "# Print the difference between the original and perturbated camera pose\n",
    "get_world_frame_difference(T_inW_ofC, T_inW_ofC_perturbated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the image at the initial guess; this will serve as a reference image (with a known pose) against which the query performs feature matching to estimate its pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create camera object\n",
    "camera = new_cameras(\n",
    "    poses=T_inW_ofC_perturbated,\n",
    "    intrinsics=np.array([K[0,0], K[1,1], K[0,2], K[1,2]], dtype=np.float32),\n",
    "    image_sizes=np.array([w, h], dtype=np.int32),\n",
    "    camera_models=np.array(camera_model_to_int(\"pinhole\"), dtype=np.int32),\n",
    ")\n",
    "\n",
    "tic = time.time()\n",
    "# Render the image\n",
    "outputs = model.render(camera=camera, options={\"outputs\": \"depth\", \"output_type_dtypes\": {'color': 'uint8', 'depth': 'float32'}})\n",
    "toc = time.time()\n",
    "print(f\"Rendering time: {toc - tic:.2f} seconds\")\n",
    "\n",
    "# print(camera)\n",
    "# print(outputs.keys())\n",
    "\n",
    "im_reference = outputs[\"color\"]\n",
    "depth_reference = outputs[\"depth\"]\n",
    "\n",
    "# Show images side by side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].imshow(im_query)\n",
    "ax[0].set_title(\"Query Image (Original)\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(im_reference)\n",
    "ax[1].set_title(\"Reference Image (Synthetic)\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform SIFT feature matching between the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature matching\n",
    "tic = time.time()\n",
    "pts_query_raw_cv, pts_reference_raw_cv, matches = do_feature_matching_SIFT(im_query, im_reference, ratio=0.7, do_visualize=True)\n",
    "toc = time.time()\n",
    "\n",
    "print(f\"Elapsed time: {toc - tic:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform  SuperPoint and SuperGlue for feature matching between the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature matching\n",
    "tic = time.time()\n",
    "pts_query_raw_cv, pts_reference_raw_cv, matches = do_feature_matching_SPSG(im_query, im_reference, superpoint_threshold=0.01, superglue_threshold=0.5, do_visualize=True)\n",
    "toc = time.time()\n",
    "\n",
    "print(f\"Elapsed time: {toc - tic:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform LoFTR for feature matching between the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature matching\n",
    "tic = time.time()\n",
    "pts_query_raw_cv, pts_reference_raw_cv, matches = do_feature_matching_LoFTR(im_query, im_reference, do_visualize=True)\n",
    "toc = time.time()\n",
    "\n",
    "print(f\"Elapsed time: {toc - tic:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "\n",
    "pts_query, pts_reference, pts_query_cv, pts_reference_cv = get_2d_feature_points(matches, pts_query_raw_cv, pts_reference_raw_cv)\n",
    "p_inW, p_inC = compute_3d_points(pts_reference, depth_reference, K, T_inW_ofC_perturbated, depth_scale=4.0)\n",
    "rvec, tvec, inliers = estimate_camera_pose(p_inW, pts_query, K, reprojection_error=5.0, iterations=50)\n",
    "\n",
    "toc = time.time()\n",
    "print(f\"Elapsed time: {toc - tic:.2f} seconds\")\n",
    "\n",
    "# visualize_feature_points(im_query, im_reference, pts_query, pts_reference, inliers)\n",
    "# visualize_matches(im_query, im_reference, pts_query_raw_cv, pts_reference_raw_cv, [matches[i] for i in inliers])\n",
    "# Optionally visualize 3D points\n",
    "# visualize_3d_points(p_inC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Reproject 3D points using estimated pose\n",
    "pts_reprojected, _ = cv.projectPoints(p_inW, rvec, tvec, K, None)\n",
    "\n",
    "# Convert to numpy array for easier processing\n",
    "pts_reprojected = pts_reprojected.reshape(-1, 2)\n",
    "\n",
    "# Step 2: Compute Reprojection Error\n",
    "reprojection_errors = np.linalg.norm(pts_query - pts_reprojected, axis=1)\n",
    "\n",
    "# Compute Mean and Median Reprojection Error\n",
    "mean_reprojection_error = np.nanmean(reprojection_errors)\n",
    "median_reprojection_error = np.nanmedian(reprojection_errors)\n",
    "\n",
    "print(f\"Mean Reprojection Error: {mean_reprojection_error:.2f} pixels\")\n",
    "print(f\"Median Reprojection Error: {median_reprojection_error:.2f} pixels\")\n",
    "\n",
    "plt.hist(reprojection_errors, bins=30, edgecolor='black')\n",
    "plt.xlabel(\"Reprojection Error (pixels)\")\n",
    "plt.ylabel(\"Number of Points\")\n",
    "plt.title(\"Reprojection Error Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct estimated transformation matrix\n",
    "T_inC_ofW_estimated = np.eye(4, dtype=np.float32)\n",
    "T_inC_ofW_estimated[:3, :3] = R.from_rotvec(rvec.flatten()).as_matrix()\n",
    "T_inC_ofW_estimated[:3, 3] = tvec.flatten()\n",
    "\n",
    "T_inW_ofC_estimated = np.linalg.inv(T_inC_ofW_estimated)\n",
    "\n",
    "# Print the difference between the two poses\n",
    "get_world_frame_difference(T_inW_ofC, T_inW_ofC_perturbated)\n",
    "get_world_frame_difference(T_inW_ofC, T_inW_ofC_estimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Superimpose the query image and the image rendered at T_inW_ofC_estimated\n",
    "\n",
    "# Create camera object\n",
    "camera = new_cameras(\n",
    "    poses=T_inW_ofC_estimated,\n",
    "    intrinsics=np.array([K[0,0], K[1,1], K[0,2], K[1,2]], dtype=np.float32),\n",
    "    image_sizes=np.array([w, h], dtype=np.int32),\n",
    "    camera_models=np.array(camera_model_to_int(\"pinhole\"), dtype=np.int32),\n",
    ")\n",
    "\n",
    "# Render the image\n",
    "outputs = model.render(camera=camera, options={\"outputs\": \"depth\", \"output_type_dtypes\": {'color': 'uint8', 'depth': 'float32'}})\n",
    "\n",
    "im_estimated = outputs[\"color\"]\n",
    "depth_estimated = outputs[\"depth\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# im_query has alpha channel, so we need to remove it\n",
    "if im_query.shape[-1] == 4:\n",
    "    im_query_copy = cv.cvtColor(im_query, cv.COLOR_RGBA2RGB)\n",
    "    # Change the background to white\n",
    "    im_query_copy[im_query[..., 3] == 0] = 255\n",
    "else:\n",
    "    im_query_copy = im_query\n",
    "\n",
    "ax[0].imshow(im_query_copy)\n",
    "ax[0].set_title(\"Query Image\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(im_reference)\n",
    "ax[1].set_title(\"Rendered Image (Initial Guess)\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].imshow(im_estimated)\n",
    "ax[2].set_title(\"Rendered Image (Estimate)\")\n",
    "ax[2].axis(\"off\")\n",
    "\n",
    "# Blend the images\n",
    "im_blended = cv.addWeighted(im_query_copy, 0.5, im_estimated, 0.5, 0)\n",
    "\n",
    "# Display the blended image\n",
    "ax[3].imshow(im_blended)\n",
    "ax[3].set_title(\"Blended Image (Query + Estimate)\")\n",
    "ax[3].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, put all the things in a single code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 12.10 # Rotation perturbation in degrees\n",
    "N = 0.87  # Translation perturbation in meters\n",
    "\n",
    "num_iterations = 20  # Number of iterations for the loop\n",
    "rot_errors = [None] * num_iterations\n",
    "trs_errors = [None] * num_iterations\n",
    "elapsed_times = [None] * num_iterations\n",
    "\n",
    "# For loop starts here ...\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    filename = rng.choice(filenames)\n",
    "    print(f\"Selected frame: {filename}\")\n",
    "    im_query = np.array(Image.open(Path(scene_path, reading_dir, filename + \".JPG\"))) if dataset == \"mipnerf360\" else \\\n",
    "        np.array(Image.open(Path(scene_path, reading_dir, filename + \".png\")))\n",
    "    T_inW_ofC = frames[filename]\n",
    "\n",
    "    tic = time.time()\n",
    "\n",
    "    # Create random pose perturbation and render a reference image\n",
    "    T_inW_ofC_perturbated = perturb_SE3(T_inW_ofC, \n",
    "                                        rotation_magnitude=M, \n",
    "                                        translation_magnitude=N, \n",
    "                                        axis_mode='random', magnitude_mode='gaussian', \n",
    "                                        rng=rng)\n",
    "\n",
    "    camera = new_cameras(\n",
    "        poses=T_inW_ofC_perturbated,\n",
    "        intrinsics=np.array([K[0,0], K[1,1], K[0,2], K[1,2]], dtype=np.float32),\n",
    "        image_sizes=np.array([w, h], dtype=np.int32),\n",
    "        camera_models=np.array(camera_model_to_int(\"pinhole\"), dtype=np.int32),\n",
    "    )\n",
    "    outputs = model.render(camera=camera, options={\"outputs\": \"depth\", \"output_type_dtypes\": {'color': 'uint8', 'depth': 'float32'}})\n",
    "    im_reference = outputs[\"color\"]\n",
    "    depth_reference = outputs[\"depth\"]\n",
    "\n",
    "    # Do feature detection and matching (LoFTR) on both the query and the reference image\n",
    "    pts_query_raw_cv, pts_reference_raw_cv, matches = do_feature_matching_SPSG(im_query, im_reference, do_visualize=False)\n",
    "\n",
    "    # Do pose estimation\n",
    "    pts_query, pts_reference, pts_query_cv, pts_reference_cv = get_2d_feature_points(matches, pts_query_raw_cv, pts_reference_raw_cv)\n",
    "    \n",
    "    # Check all the shapes\n",
    "    assert pts_query.shape[0] == pts_reference.shape[0], \"Number of points do not match\"\n",
    "    assert len(pts_query_cv) == len(pts_reference_cv) == len(matches), \"Number of points do not match\"\n",
    "\n",
    "    # Ensure there are enough matches for pose estimation\n",
    "    if len(matches) >= 4:\n",
    "        # Compute 3D points in the world and camera frames\n",
    "        p_inW, p_inC = compute_3d_points(\n",
    "        pts_reference, depth_reference, K, T_inW_ofC_perturbated, depth_scale=4.0\n",
    "        )\n",
    "        \n",
    "        # Estimate the camera pose using solvePnPRansac\n",
    "        rvec, tvec, inliers = estimate_camera_pose(\n",
    "        p_inW, pts_query, K, reprojection_error=5.0, iterations=50\n",
    "        )\n",
    "\n",
    "        if inliers is not None and len(inliers) > 0:\n",
    "            # Construct the estimated transformation matrix\n",
    "            T_inC_ofW_estimated = np.eye(4, dtype=np.float32)\n",
    "            T_inC_ofW_estimated[:3, :3] = R.from_rotvec(rvec.flatten()).as_matrix()\n",
    "            T_inC_ofW_estimated[:3, 3] = tvec.flatten()\n",
    "\n",
    "            # Compute the inverse to get the camera-to-world transformation\n",
    "            T_inW_ofC_estimated = np.linalg.inv(T_inC_ofW_estimated)\n",
    "        else:\n",
    "            # Fallback to the perturbated pose if pose estimation fails\n",
    "            print(f\"Pose estimation failed for frame {filename}. Using perturbated pose.\")\n",
    "            T_inW_ofC_estimated = T_inW_ofC_perturbated\n",
    "    else:\n",
    "        print(f\"Not enough matches for frame {filename}. Skipping this frame.\")\n",
    "        T_inW_ofC_estimated = T_inW_ofC_perturbated\n",
    "\n",
    "    toc = time.time()\n",
    "\n",
    "    # Report the error\n",
    "    T_inC_ofW_estimated = np.eye(4, dtype=np.float32)\n",
    "    T_inC_ofW_estimated[:3, :3] = R.from_rotvec(rvec.flatten()).as_matrix()\n",
    "    T_inC_ofW_estimated[:3, 3] = tvec.flatten()\n",
    "    T_inW_ofC_estimated = np.linalg.inv(T_inC_ofW_estimated)\n",
    "\n",
    "    get_world_frame_difference(T_inW_ofC, T_inW_ofC_perturbated)\n",
    "    rot_error, trs_error = get_world_frame_difference(T_inW_ofC, T_inW_ofC_estimated)\n",
    "    \n",
    "    # print(f\"Elapsed time: {toc - tic:.2f} seconds\")\n",
    "\n",
    "    rot_errors[i] = rot_error\n",
    "    trs_errors[i] = trs_error\n",
    "    elapsed_times[i] = toc - tic\n",
    "    print(f\"Iteration {i+1}/{num_iterations}: Rotation Error = {rot_error:.2f} degrees, Translation Error = {trs_error:.2f} meters, Elapsed Time = {toc - tic:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average errors and elapsed time\n",
    "avg_rot_error = np.nanmean(rot_errors)\n",
    "avg_trs_error = np.nanmean(trs_errors)\n",
    "avg_elapsed_time = np.nanmean(elapsed_times)\n",
    "print(f\"\\nAverage Rotation Error: {avg_rot_error:.2f} degrees\")\n",
    "print(f\"Average Translation Error: {avg_trs_error:.2f} meters\")\n",
    "print(f\"Average Elapsed Time: {avg_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of outcomes with <5deg and <0.05m criteria\n",
    "count_5deg = np.sum(np.array(rot_errors) < 5)\n",
    "count_5cm = np.sum(np.array(trs_errors) < 0.05)\n",
    "print(f\"Number of outcomes with <5deg: {count_5deg} (ratio: {count_5deg / len(rot_errors):.2f})\")\n",
    "print(f\"Number of outcomes with <0.05m: {count_5cm} (ratio: {count_5deg / len(trs_errors):.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutorial, we used `ExitStack` to simplify context management. \n",
    "# We need to close the context to release the memory.\n",
    "stack.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
